{"cells":[{"cell_type":"markdown","metadata":{"id":"PaiG8Ulc75xc"},"source":["# ðŸ¦‰ OWL-ViT inference playground\n","\n","OWL-ViT is an **open-vocabulary object detector**. Given a free-text query, it will find objects matching that query. It can also do **one-shot object detection**, i.e. detect objects based on a single example image.\n","\n","This Colab allows you to query the model interactively, to get a feeling for its capabilities. For details on the model, check out the [paper](https://arxiv.org/abs/2205.06230) or the [code](https://github.com/google-research/scenic/tree/main/scenic/projects/owl_vit).\n","\n","> â— Note: The free public Colab runtime has enough memory for the ViT-B/16 model. For optimal results, use a Pro or local runtime and the ViT-L/14 model.\n","\n","> â— Note: This Colab is optimized for fast interactive exploration. It does not apply some of the optimizations and augmentations that would be used in a rigorous evaluation settings, so results from this Colab may not match the paper.\n","\n","## How to use this Colab\n","1. Use a GPU or TPU Colab runtime.\n","2. Run all cells in the Colab from top to bottom.\n","3. Go to the cells for [Text-conditioned object detection](#scrollTo=aNzcyP1sbJ9w&uniqifier=1) or [Image-conditioned object detection](#scrollTo=TFlZhrDTQbiY&uniqifier=1) and have fun!\n","\n","**If you run into any problems, please [file an issue](https://github.com/google-research/scenic/issues/new?title=OWL-ViT+inference+playround:+[add+title]) on GitHub.**\n","\n"]},{"cell_type":"markdown","metadata":{"id":"5-Yta1B7rtWu"},"source":["# Setup\n","\n","OWL-ViT is implemented in [Scenic](https://github.com/google-research/scenic). The cell below installs the Scenic codebase from GitHub and imports it."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ywzNXEFqlFD5"},"outputs":[],"source":["from google.colab import drive\n","drive.flush_and_unmount()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZgIVOZqkVlSU"},"outputs":[],"source":["# colab workspaces dont have a lot of fonts installed, and this one is pretty nice\n","!curl -L -O https://github.com/source-foundry/Hack/releases/download/v3.003/Hack-v3.003-ttf.tar.gz\n","!tar -xzvf Hack-v3.003-ttf.tar.gz\n","!mkdir /usr/share/fonts/truetype/ttf\n","!mv ttf/Hack-Regular.ttf /usr/share/fonts/truetype/ttf/Hack-Regular.ttf"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WuXTPatiZkEv"},"outputs":[],"source":["import os\n","if 'COLAB_GPU' in os.environ:\n","  print(\"I'm running on Colab\")\n","else:\n","  print(\"Running this notebook on your own pc is dangerous due to the setup of scenic (which i didnt mess with)... Use colab or a docker container[if using docker, remove these few lines]\")\n","  exit()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zWF7RkeZ4B_N"},"outputs":[],"source":["if 'Cloned' not in globals(): # Only clone once (this is time-consuming, and unnecessary if already done)\n","  !rm -rf *\n","  !rm -rf .config\n","  !rm -rf .git\n","  !git clone https://github.com/google-research/scenic.git .\n","  !python -m pip install -q .\n","  !python -m pip install -r ./scenic/projects/owl_vit/requirements.txt\n","\n","  # Also install big_vision, which is needed for the mask head:\n","  !mkdir /big_vision\n","  !git clone https://github.com/google-research/big_vision.git /big_vision\n","  !python -m pip install -r /big_vision/big_vision/requirements.txt\n","  import sys\n","  sys.path.append('/big_vision/')\n","  !echo \"Done.\"\n","  Cloned = True"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xb6czshVUCei"},"outputs":[],"source":["!python -m pip install brambox"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E3C3ROFqU5UJ"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xlp0LYJKU2iE"},"outputs":[],"source":["annotations_folder_path: str = \"./drive/MyDrive/AI/Data/Annotations/\"\n","images_folder_path: str = \"./drive/MyDrive/AI/Data/JPEGImages/\"\n","fixed_images_path: str = \"./drive/MyDrive/AI/Data/Fixed_support/\"\n","\n","output_path_base: str = \"./drive/MyDrive/AI/Outputs/\"\n","saves_path_base: str= \"./drive/MyDrive/AI/Saves/\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9MKZb6G3-H92"},"outputs":[],"source":["import os\n","\n","from bokeh import io as bokeh_io\n","import jax\n","from google.colab import output as colab_output\n","import matplotlib as mpl\n","from matplotlib import pyplot as plt\n","import numpy as np\n","import pandas as pd\n","from PIL import Image, ImageFont, ImageDraw\n","from scenic.projects.owl_vit import models\n","from scenic.projects.owl_vit.configs import clip_l14 as config_module\n","from scenic.projects.owl_vit.notebooks import inference\n","from scenic.projects.owl_vit.notebooks import interactive\n","from scipy.special import expit as sigmoid\n","import skimage\n","from skimage import io as skimage_io\n","from skimage import transform as skimage_transform\n","import tensorflow as tf\n","\n","import brambox as bb\n","import itertools\n","\n","tf.config.experimental.set_visible_devices([], 'GPU')\n","bokeh_io.output_notebook(hide_banner=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3Gh3U4xSVa6b"},"outputs":[],"source":["class BBAnnotatedImage:\n","    def __init__(self, *, img_dir = \"\", filename= \"\", annotations:pd.DataFrame, scale=1) -> None:\n","        if not filename:\n","            raise ValueError(\"filename cannot be empty\")\n","        if not os.path.exists(os.path.join(img_dir, filename)):\n","            raise FileNotFoundError(f\"image file {filename} not found in {img_dir}\")\n","        self.filename: str = filename\n","        self.image: Image.Image = Image.open(os.path.join(img_dir, filename))\n","        self.annotations: pd.DataFrame = annotations\n","        # if height > width rotate image and annotations to get all landscape img\n","        if self.height > self.width:\n","            width = self.width\n","            self.image = self.image.rotate(90, expand=True)\n","            new_annotations = pd.DataFrame(columns = self.annotations.columns)\n","            for _, annotation in self.annotations.iterrows():\n","                annotation['x_top_left'], annotation['y_top_left'] = annotation['y_top_left'], width - annotation['x_top_left'] - annotation['width']\n","                annotation['width'], annotation['height'] = annotation['height'], annotation['width']\n","                new_annotations = pd.concat([new_annotations, annotation.to_frame().T])\n","            self.annotations = new_annotations\n","\n","        width = self.width // scale\n","        height = self.height // scale\n","        self.image = self.image.resize((width, height))\n","        new_annotations = pd.DataFrame(columns = self.annotations.columns)\n","        for _, annotation in self.annotations.iterrows():\n","          annotation['x_top_left'], annotation['y_top_left'] = annotation['x_top_left'] // scale, annotation['y_top_left'] // scale\n","          annotation['width'], annotation['height'] = annotation['width'] // scale, annotation['height'] // scale\n","          new_annotations = pd.concat([new_annotations, annotation.to_frame().T])\n","        self.annotations = new_annotations\n","\n","\n","    @property\n","    def width(self) -> int:\n","        return self.image.width\n","\n","    @property\n","    def height(self) -> int:\n","        return self.image.height\n","\n","    @staticmethod\n","    def from_xml(xml_file:str, img_dir:str = \"\", scale=1) -> 'BBAnnotatedImage':\n","        annotations:pd.DataFrame = bb.io.load('anno_pascalvoc', xml_file)\n","        filename = annotations.iloc[[0]]['image'].values[0] + '.jpg'\n","        return BBAnnotatedImage(filename = filename, img_dir = img_dir, annotations = annotations, scale=scale)\n","\n","    def get_object_image_cutout(self, annotation_row:pd.Series) -> Image.Image:\n","        return self.image.crop((annotation_row['x_top_left'], annotation_row['y_top_left'], annotation_row['x_top_left'] + annotation_row['width'], annotation_row['y_top_left'] + annotation_row['height']))\n","\n","    @property\n","    def annotation_classes(self) -> list[str]:\n","        return self.annotations['class_label'].tolist()\n","\n","    def drawn_annotations(self, resize=10) -> Image.Image:\n","        img = self.image.copy()\n","        draw = ImageDraw.Draw(img)\n","        for _, annotation in self.annotations.iterrows():\n","            draw.rectangle((annotation['x_top_left'], annotation['y_top_left'], annotation['x_top_left'] + annotation['width'], annotation['y_top_left'] + annotation['height']), outline='red', width=20)\n","            draw.text((annotation['x_top_left'], annotation['y_top_left']), annotation['class_label'], fill=\"red\", font=ImageFont.truetype(\"arial.ttf\", 150))\n","        draw.text((10, 10), self.filename, fill=\"red\", font=ImageFont.truetype(\"arial.ttf\", 150))\n","        img = img.resize((int(img.width / resize), int(img.height / resize)))\n","        return img\n","\n","    @property\n","    def object_image_cutouts(self) -> list[Image.Image]:\n","        return [self.get_object_image_cutout(annotation) for _, annotation in self.annotations.iterrows()]\n","\n","    @property\n","    def objects(self) -> list[tuple[str, Image.Image]]:\n","        return list(zip(self.annotation_classes, self.object_image_cutouts))\n","\n","    @property\n","    def npimage(self):\n","        return np.asarray(self.image)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IaWHOCZObaOF"},"outputs":[],"source":["def create_folder_if_not_exists(folder_name):\n","    if not os.path.exists(folder_name):\n","        os.makedirs(folder_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QNb3b_25VvhN"},"outputs":[],"source":["def bb_load_dataset(annotations_folder:str, images_folder:str, area_column:bool=True, scale = 1) -> list[BBAnnotatedImage]:\n","    _, _, files = next(os.walk(annotations_folder))\n","    annotated_images:list[BBAnnotatedImage] = []\n","    for file in files:\n","        annotated_images.append(BBAnnotatedImage.from_xml(annotations_folder + file, images_folder, scale=scale))\n","    if area_column:\n","        # add area column\n","        for annotated_image in annotated_images:\n","            for _, annotation in annotated_image.annotations.iterrows():\n","                annotated_image.annotations.loc[_, 'area'] = annotation['width'] * annotation['height']\n","    return annotated_images\n","\n","dataset:list[BBAnnotatedImage] = bb_load_dataset(annotations_folder_path, images_folder_path, scale=4)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dcO3mY5AMRUs"},"outputs":[],"source":["def filter_by_confidence(dataframe, minimum_score:float):\n","  return dataframe.loc[dataframe['confidence'] > minimum_score]\n","\n","def get_detections_for_image(detections, image_name):\n","    return detections.loc[detections['image'] == image_name]\n","\n","def filter_by_iou(dataframe, iou:float):\n","  return dataframe.loc[dataframe['iou'] < iou]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4zMqjLApZR71"},"outputs":[],"source":["def draw_boxes(image:BBAnnotatedImage, data:pd.DataFrame, alligntop=True, draw_legend=True, line=True, manual_image:Image=None, forced_color = False):\n","    image_name = image.filename.split('.')[0]\n","    boxed = image.image.copy()\n","    if manual_image != None:\n","      boxed = manual_image\n","    boxed = boxed.convert(\"RGBA\")\n","    draw:ImageDraw = ImageDraw.Draw(boxed)\n","    data = get_detections_for_image(data, image_name)\n","    default_colors = [\n","        (31, 119, 180),\n","        (255, 127, 14),\n","        (44, 160, 44),\n","        (214, 39, 40),\n","        (148, 103, 189),\n","        (140, 86, 75),\n","        (227, 119, 194),\n","        (127, 127, 127),\n","        (188, 189, 34),\n","        (23, 190, 207),\n","    ]\n","    colors_assigned = dict(zip(annotations['class_label'].unique(), default_colors))\n","    # map all detections to colors inplace in detections\n","    #data['color'] = data['class_label'].map(colors_assigned)\n","\n","    if line:\n","        # make all colors darker\n","        #data['color'] = data['color'].apply(lambda x: tuple([int(y * 0.9) for y in x]))\n","        for index, row in data.iterrows():\n","            color=colors_assigned[row['class_label']]\n","            if forced_color:\n","              color = (0, 0, 0)\n","            #draw.rectangle([row['x_top_left'] * boxed.width, row['y_top_left'] * boxed.height,(row['x_top_left'] + row['width']) * boxed.width, (row['y_top_left'] + row['height']) * boxed.height], outline=color)\n","            draw.rectangle([row['x_top_left'], row['y_top_left'], (row['x_top_left'] + row['width']), (row['y_top_left'] + row['height'])], outline=color, width=5)\n","    else:\n","        for index, row in data.iterrows():\n","            color=colors_assigned[row['class_label']]\n","            overlay = Image.new('RGBA', boxed.size, (0,0,0,0))\n","            draw_overlay = ImageDraw.Draw(overlay)  # Create a context for drawing things on it.\n","            draw_overlay.rectangle([row['x_top_left'], row['y_top_left'], (row['x_top_left'] + row['width']), (row['y_top_left'] + row['height'])], fill=color+(102,))\n","            boxed = Image.alpha_composite(boxed, overlay)\n","    if draw_legend:\n","    # write legend in top left corner\n","        if alligntop:\n","            legend_x = 10\n","            legend_y = 10\n","            legend_size = image.height // 20\n","            for class_label, color in colors_assigned.items():\n","                draw.rectangle([legend_x, legend_y, legend_x + legend_size, legend_y + legend_size], fill=color)\n","                draw.text((legend_x + legend_size + 10, legend_y), class_label, fill=color, font=ImageFont.truetype(\"Hack-Regular.ttf\", legend_size))\n","                legend_y += legend_size + 10\n","        else:\n","            #legend in bottom left corner\n","            legend_x = 10\n","            legend_y = boxed.height - 10\n","            legend_size = boxed.height // 20\n","            for class_label, color in colors_assigned.items().__reversed__():\n","                draw.rectangle([legend_x, legend_y - legend_size, legend_x + legend_size, legend_y], fill=color)\n","                draw.text((legend_x + legend_size + 10, legend_y - legend_size), class_label, fill=color, font=ImageFont.truetype(\"arial.ttf\", legend_size))\n","                legend_y -= legend_size + 10\n","    boxed = boxed.convert(\"RGB\")\n","    return boxed\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9boPSgIvk4nL"},"outputs":[],"source":["def calc_iou(detections, image_boxes) -> None: #inplace\n","    # image_boxes: Dataframe with row for each box in the image ([x_top_left, y_top_left, width, height, confidence, iou])\n","    # fill iou row inplace\n","    indices = image_boxes.index.values.tolist()\n","    for i in range(len(image_boxes)):\n","        # get box i\n","        box_i = detections.loc[indices[i]]\n","        # get coordinates of box i\n","        x1_i = box_i['x_top_left']\n","        y1_i = box_i['y_top_left']\n","        x2_i = x1_i + box_i['width']\n","        y2_i = y1_i + box_i['height']\n","        # get area of box i\n","        area_i = box_i['width'] * box_i['height']\n","        # get confidence of box i\n","        conf_i = box_i['confidence']\n","        # compare box i with all other boxes\n","        for j in range(i + 1, len(image_boxes)):\n","            # get box j\n","            box_j = detections.loc[indices[j]]\n","            # get confidence of box j\n","            conf_j = box_j['confidence']\n","            # get coordinates of box j\n","            x1_j = box_j['x_top_left']\n","            y1_j = box_j['y_top_left']\n","            x2_j = x1_j + box_j['width']\n","            y2_j = y1_j + box_j['height']\n","            # get area of box j\n","            area_j = box_j['width'] * box_j['height']\n","            # compute intersection\n","            x1_inter = max(x1_i, x1_j)\n","            y1_inter = max(y1_i, y1_j)\n","            x2_inter = min(x2_i, x2_j)\n","            y2_inter = min(y2_i, y2_j)\n","            # check if there is an intersection\n","            if x1_inter > x2_inter or y1_inter > y2_inter:\n","              continue\n","            # compute area of intersection\n","            area_inter = (x2_inter - x1_inter) * (y2_inter - y1_inter)\n","            # compute iou\n","            iou = area_inter / (area_i + area_j - area_inter)\n","            # update iou value of box i/j\n","            if conf_j > conf_i:\n","              if iou > box_i['iou']:\n","                detections.loc[indices[i], 'iou'] = iou\n","            else:\n","              if iou > box_j['iou']:\n","                detections.loc[indices[j], 'iou'] = iou\n"]},{"cell_type":"markdown","metadata":{"id":"EnD94y6ia6Mn"},"source":["## Set up the model\n","This takes a minute or two."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1UiX2Nx8auW4"},"outputs":[],"source":["config = config_module.get_config(init_mode='canonical_checkpoint')\n","module = models.TextZeroShotDetectionModule(\n","    body_configs=config.model.body,\n","    normalize=config.model.normalize,\n","    box_bias=config.model.box_bias)\n","variables = module.load_variables(config.init_from.checkpoint_path)\n","model = inference.Model(config, module, variables)\n","model.warm_up()"]},{"cell_type":"code","source":["#this uses the global model so yeah its here\n","\n","def get_image_query_embedding(image:BBAnnotatedImage, annotation_index:int):\n","  _, annotation = list(image.annotations.iterrows())[annotation_index]\n","  y_min = annotation.y_top_left\n","  x_min = annotation.x_top_left\n","  y_max = y_min + annotation.height\n","  x_max = x_min + annotation.width\n","\n","  y_min_norm = y_min / image.width # all width because width is greatest and internally img is padded\n","  x_min_norm = x_min / image.width\n","  y_max_norm = y_max / image.width\n","  x_max_norm = x_max / image.width\n","\n","  query_embeddings, _ = model.embed_image_query(image.npimage, (y_min_norm, x_min_norm, y_max_norm, x_max_norm))\n","\n","  return annotation.class_label, query_embeddings\n","\n","def get_all_query_embeddings(image:BBAnnotatedImage) -> list(tuple((str, np.ndarray))):\n","  results = []\n","  for i in range(len(image.annotations)):\n","    results.append(get_image_query_embedding(image, i))\n","  return results\n","\n","def get_all_query_embeddings_stubs(image:BBAnnotatedImage):\n","  results = []\n","  for i in range(len(image.annotations)):\n","    _, annotation = list(image.annotations.iterrows())[i]\n","    results.append((annotation.class_label, (0)))\n","  return results"],"metadata":{"id":"AD3oeQ_Zxnxz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B8f1e08EG60Y"},"source":["# Set run-type"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D6C4C8-kxrAO"},"outputs":[],"source":["use_text_queries = False\n","no_inference = True\n","\n","classless = False\n","\n","use_negative_queries = False #text only\n","\n","N = 20 #image only\n","use_all_generated_image_embeddings = False  # we eliminate images we take our query embeddings from anyways, might aswell use them completely\n","\n","\n","class_string = \"classless\" if classless else \"classed\"\n","neg_string = \"use_negative_queries\" if use_negative_queries else \"only_positive_queries\"\n","text_string = \"text\" if use_text_queries else \"img\"\n","extra_img_string = f\"full_{N}_\" if (use_all_generated_image_embeddings) else f\"{N}_\"\n","extra_img_string = extra_img_string if not use_text_queries else \"\"\n","\n","output_path = f\"{output_path_base}/owlvit/{text_string}/{class_string}/{neg_string}/{extra_img_string}\"\n","create_folder_if_not_exists(output_path)\n","\n","saves_path = f\"{saves_path_base}/owlvit/{text_string}/{class_string}/{neg_string}/{extra_img_string}\"\n","create_folder_if_not_exists(saves_path)"]},{"cell_type":"markdown","source":["### Set annotations, dataset and embeddings"],"metadata":{"id":"i1gmawgvSByD"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"H89dbh3PIReP"},"outputs":[],"source":["if use_text_queries:\n","  annotations:pd.DataFrame = pd.concat([image.annotations for image in dataset])\n","\n","  if classless:\n","    positive_text_queries = (\"shell\", \"seashell\") + tuple(annotations.class_label.unique())\n","  else:\n","    positive_text_queries = tuple(annotations.class_label.unique())\n","  if use_negative_queries:\n","    negative_text_queries = (\"rock\", \"sand\")\n","  else:\n","    negative_text_queries = ()\n","  queries = negative_text_queries + positive_text_queries\n","  print(queries)\n","  query_embeddings = model.embed_text_queries(queries)\n","\n","  test_dataset = dataset\n","\n","else:\n","  annotations:pd.DataFrame = pd.concat([image.annotations for image in dataset])\n","\n","  test_dataset = dataset.copy()\n","  print(test_dataset)\n","  query_dataset = set()\n","\n","  # we take out the query images from which we make the query embeddings\n","  # if a class has less than N annotations we dont use it\n","  annotations_per_class = annotations['class_label'].value_counts()\n","  print(annotations_per_class)\n","  removed_classes = []\n","  queries_dict = {}\n","  for annotation_class, _ in annotations_per_class.items():\n","    queries_dict[annotation_class] = []\n","  for annotation_class, amount in reversed(list(annotations_per_class.items())): #loop over counts in reverse\n","    print(amount)\n","    if amount < N:\n","      print(f\"Removing: {annotation_class} as it has less annotations than N({N})\")\n","      removed_classes.append(annotation_class)\n","    else:\n","      if len(queries_dict[annotation_class]) > N:\n","        print(f\"Skipping: {annotation_class} as it is already populated with more embeddings than N({N})\")\n","        continue #we dont need to generate any more queries for classes that are already sufficiently populated\n","\n","      print(f\"Generating query embeddings for {annotation_class}\")\n","      for image in test_dataset:\n","        if annotation_class in list(image.annotations.class_label.values):\n","          query_dataset.add(image)\n","          if no_inference:\n","            img_query_embeddings = get_all_query_embeddings_stubs(image)\n","          else:\n","            img_query_embeddings = get_all_query_embeddings(image)\n","          for query_class, embedding in img_query_embeddings:\n","            queries_dict[query_class].append(embedding)\n","        if len(queries_dict[annotation_class]) > N:\n","          print(f\"Done: {annotation_class} is populated with more embeddings than N({N})\")\n","          break\n","\n","  for image in query_dataset:\n","    test_dataset.remove(image)\n","\n","  annotations = pd.concat([image.annotations for image in test_dataset])\n","\n","  for c in removed_classes:\n","    annotations = annotations[annotations['class_label'] != c]\n","    queries_dict.pop(c)\n","\n","\n","  query_embeddings_list = []\n","  queries = []\n","\n","  for key, value in queries_dict.items():\n","    print(key, len(value))\n","    queries.append(key)\n","    if use_all_generated_image_embeddings:\n","      query_embeddings_list.append(np.mean(value, axis=0))\n","    else:\n","      query_embeddings_list.append(np.mean(value[:N], axis=0))\n","\n","  query_embeddings = np.array(query_embeddings_list)\n","\n","\n","\n","if classless:\n","    annotations['class_label'] = 'object'\n","\n","lowest_confidence = 0.10 if classless else 0.05\n","lowest_confidence = lowest_confidence if use_text_queries else 0.6"]},{"cell_type":"markdown","source":["# Run"],"metadata":{"id":"h4CwJ5xfXHC0"}},{"cell_type":"markdown","metadata":{"id":"B01EUf0pyJpX"},"source":["## Inference"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8teG83eKbNKl"},"outputs":[],"source":["if use_text_queries:\n","  # This isnt all that useful, but it is used to count the amount of boxes that the model gives.\n","  # Could this be done with our own images or even a null-image?\n","  # Sure, but why bother if it already works (doesnt take that long either)\n","\n","  IMAGE_DIR = 'gs://scenic-bucket/owl_vit/example_images'\n","  %matplotlib inline\n","\n","  images = {}\n","\n","  for i, filename in enumerate(tf.io.gfile.listdir(IMAGE_DIR)):\n","    with tf.io.gfile.GFile(os.path.join(IMAGE_DIR, filename), 'rb') as f:\n","      image = mpl.image.imread(\n","          f, format=os.path.splitext(filename)[-1])[..., :3]\n","    if np.max(image) <= 1.:\n","      image *= 255\n","    images[i] = image\n","  IMAGE_ID =   2\n","  image = images[IMAGE_ID]\n","\n","  _, _, boxes = model.embed_image(image)\n","\n","else:\n","  test_image_annotated = dataset[1]\n","  _, _, boxes = model.embed_image(test_image_annotated.npimage)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7_PvQGwnWIl6"},"outputs":[],"source":["detections:pd.DataFrame = pd.DataFrame(index=range(len(test_dataset) * len(boxes)), columns=['image', 'class_label', 'id', 'x_top_left', 'y_top_left', 'width', 'height', 'confidence', 'area'])\n","\n","index = 0\n","\n","for q in range(len(test_dataset)):\n","    print(f\"{q+1}/{len(test_dataset)}\")\n","# for q in range(1):\n","    test_image_annotated = test_dataset[q]\n","    test_image_name = test_image_annotated.filename.split(\".\")[0]\n","    test_image = test_image_annotated.npimage\n","\n","    _, _, boxes = model.embed_image(test_image)\n","    query_index, scores = model.get_scores(test_image, query_embeddings, len(queries))\n","\n","    zipped_scores = zip(query_index, scores, boxes) # tuples: (label_id, score, [x, y, w, h])\n","    for i in range(len(boxes)):\n","        box = boxes[i]\n","        score = scores[i]\n","        class_label = queries[query_index[i]]\n","        x_top_left, y_top_left, width, height = box\n","        detections.loc[index] = [test_image_name, class_label, i, ((x_top_left - width/2) * test_image_annotated.width), ((y_top_left - height/2) * test_image_annotated.width) , width * test_image_annotated.width, height * test_image_annotated.width, score, width * height * test_image_annotated.height * test_image_annotated.width]\n","        index += 1\n","\n","        #new_row:pd.DataFrame = pd.DataFrame(columns=detections.columns, data=[[test_image_name, class_label, i, x_top_left, y_top_left, width, height, score]])\n","\n","        #detections.iloc[q*len(queries)+i] = new_row\n","\n","# add area column\n","detections['image'] = detections['image'].astype('category')\n","detections['area'] = detections['width'] * detections['height']\n","\n","if use_negative_queries and use_text_queries:\n","  for q in negative_text_queries:\n","      detections = detections[detections['class_label'] != q]\n","if classless:\n","  for q in queries:\n","    detections.loc[detections['class_label'] == q, 'class_label'] = 'object'\n","\n","bb.io.save(detections, \"pandas\", saves_path + \"unprocessed_detections.pkl\")"]},{"cell_type":"markdown","metadata":{"id":"7ULHbfegGqf5"},"source":["## Post process results (add IoU)\n","\n","Yes, this is horribly inefficient and should definitely not be done in python (or be this unoptimised). But it is what it is"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ldz-s18aIy1R"},"outputs":[],"source":["detections = bb.io.load(\"pandas\", saves_path + \"unprocessed_detections.pkl\")\n","\n","detections['iou'] = 0.0\n","\n","detections = filter_by_confidence(detections, lowest_confidence)\n","\n","index = 1\n","total_len = len(detections.image.unique())\n","\n","for image in detections.image.unique():\n","  img_detections = get_detections_for_image(detections, image)\n","  print(f\"{index}/{total_len} [{len(img_detections)}]\")\n","  index += 1\n","  calc_iou(detections, img_detections)\n","\n","bb.io.save(detections, \"pandas\", saves_path + \"processed_detections.pkl\")"]},{"cell_type":"markdown","metadata":{"id":"N47t1wnEx_bi"},"source":["## Results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OJ9P0mSYMImq"},"outputs":[],"source":["detections = bb.io.load(\"pandas\", saves_path + \"processed_detections.pkl\")\n","\n","detections['height'] = detections['height'].astype(float)\n","detections['confidence'] = detections['confidence'].astype(float)\n","detections['area'] = detections['area'].astype(float)\n","detections['iou'] = detections['iou'].astype(float)\n","detections['x_top_left'] = detections['x_top_left'].astype(float)\n","detections['y_top_left'] = detections['y_top_left'].astype(float)\n","detections['width'] = detections['width'].astype(float)\n","\n","\n","annotations['x_top_left'] = annotations['x_top_left'].astype(float)\n","annotations['y_top_left'] = annotations['y_top_left'].astype(float)\n","annotations['width'] = annotations['width'].astype(float)\n","annotations['height'] = annotations['height'].astype(float)\n","annotations['occluded'] = annotations['occluded'].astype(float)\n","annotations['truncated'] = annotations['truncated'].astype(float)\n","annotations['lost'] = annotations['lost'].astype(bool)\n","annotations['difficult'] = annotations['difficult'].astype(bool)\n","annotations['ignore'] = annotations['ignore'].astype(bool)\n","\n","iou_thresholds = [round(0.1 * x, 2) for x in range(6,11)]\n","score_thresholds = [round(x/10, 6) for x in range(round(lowest_confidence * 10), 10)] #lower than the lowest score is meaninless ofc\n","\n","options_matrix = list(itertools.product(iou_thresholds, score_thresholds))\n","\n","save_images = True\n","image_start = 50\n","image_amount = 50\n","\n","image_stop = image_start + image_amount\n","if image_stop > len(test_dataset):\n","  image_stop = len(test_dataset)\n","\n","\n","\n","if save_images:\n","  for iou, score in options_matrix:\n","    det_iou = filter_by_iou(detections, iou)\n","    det = filter_by_confidence(det_iou, score)\n","\n","    img_path = f\"{output_path}iou{iou}/score{score}/\"\n","    create_folder_if_not_exists(img_path)\n","    print(f\"iou={iou}-score={score}: #detections={len(det)}\")\n","    for image in test_dataset[image_start:image_stop]:\n","      boxed:Image = draw_boxes(image, annotations, line=False)\n","      if classless:\n","        boxed:Image = draw_boxes(image, det, draw_legend=True, manual_image=boxed, forced_color = True)\n","      else:\n","        boxed:Image = draw_boxes(image, det, draw_legend=True, manual_image=boxed, forced_color = False)\n","      with open(img_path + image.filename, \"w\") as imfp:\n","        boxed.save(imfp)\n","\n","max_ap = 0\n","max_ap_nms = 0\n","max_ap_pr = None\n","max_ap_fscore = 0\n","\n","max_fscore = None\n","max_fscore_f1 = 0\n","max_fscore_nms = 0\n","max_fscore_pr = None\n","max_fscore_ap = 0\n","\n","fig, ax = plt.subplots(figsize=(10,10))\n","\n","for iou_threshold in iou_thresholds:\n","  print(iou_threshold)\n","\n","  det = filter_by_iou(detections, iou_threshold)\n","  pr = bb.stat.pr(det, annotations, threshold=0.5, smooth=True)\n","  ap = bb.stat.ap(pr)\n","  fscore = bb.stat.fscore(pr)\n","  peakf1 = bb.stat.peak(fscore)\n","  ax.plot(pr['recall'], pr['precision'], label=f\"nms={iou_threshold}, AP={100 * ap:.2f}%, F1={100 * peakf1.f1:.2f}%\")\n","\n","  print(f\"nms={iou_threshold}, AP={100 * ap:.2f}%, F1={100 * peakf1.f1:.2f}%\")\n","\n","  if ap > max_ap:\n","      max_ap = ap\n","      max_ap_nms = iou_threshold\n","      max_ap_pr = pr\n","      max_ap_fscore = peakf1.f1\n","  if peakf1.f1 > max_fscore_f1:\n","      max_fscore_f1 = peakf1.f1\n","      max_fscore = peakf1\n","      max_fscore_nms = iou_threshold\n","      max_fscore_pr = pr\n","      max_fscore_ap = ap\n","      topf1 = bb.stat.point(pr, peakf1.f1)\n","\n","plt.title(\"Precision-Recall curve for different nms values\")\n","ax.set_xlabel(\"Recall\")\n","ax.set_ylabel(\"Precision\")\n","ax.legend()\n","\n","fig.savefig(f\"{output_path}iouPR.png\")\n","plt.close()\n","\n","print(f\"best ap: {max_ap * 100}%, best fscore: {max_fscore_f1 * 100}%\")\n","print(f\"maximize ap: nms={max_ap_nms}\")\n","print(f\"maximize fscore: nms={max_fscore_nms}\")\n","\n","\n","iou_thresholds = [round(0.01 * x, 2) for x in range(1,101)]\n","\n","\n","max_ap = 0\n","max_ap_nms = 0\n","max_ap_pr = None\n","max_ap_fscore = 0\n","\n","max_fscore = None\n","max_fscore_f1 = 0\n","max_fscore_nms = 0\n","max_fscore_pr = None\n","max_fscore_ap = 0\n","\n","for iou_threshold in iou_thresholds:\n","\n","  det = filter_by_iou(detections, iou_threshold)\n","  pr = bb.stat.pr(det, annotations, threshold=0.5, smooth=True)\n","  ap = bb.stat.ap(pr)\n","  fscore = bb.stat.fscore(pr)\n","  peakf1 = bb.stat.peak(fscore)\n","  if ap > max_ap:\n","      max_ap = ap\n","      max_ap_nms = iou_threshold\n","      max_ap_pr = pr\n","      max_ap_fscore = peakf1.f1\n","  if peakf1.f1 > max_fscore_f1:\n","      max_fscore_f1 = peakf1.f1\n","      max_fscore = peakf1\n","      max_fscore_nms = iou_threshold\n","      max_fscore_pr = pr\n","      max_fscore_ap = ap\n","      topf1 = bb.stat.point(pr, peakf1.f1)\n","\n","\n","print(f\"best ap: {max_ap * 100}%, best fscore: {max_fscore_f1 * 100}%\")\n","print(f\"maximize ap: nms={max_ap_nms}\")\n","print(f\"maximize fscore: nms={max_fscore_nms}\")\n","\n","opt_det = filter_by_iou(detections, max_ap_nms)\n","\n","fig, ax = plt.subplots(figsize=(10,10))\n","\n","for shell_class in detections.class_label.unique():\n","  shell_det = opt_det.loc[opt_det['class_label'] == shell_class]\n","  shell_anno = annotations[annotations['class_label'] == shell_class]\n","  pr = bb.stat.pr(shell_det, shell_anno, threshold=0.5, smooth=True)\n","  ap = bb.stat.ap(pr)\n","  fscore = bb.stat.fscore(pr)\n","  peakf1 = bb.stat.peak(fscore)\n","  ax.plot(pr['recall'], pr['precision'], label=f\"{shell_class}: AP={100 * ap:.2f}%, F1={100 * peakf1.f1:.2f}%\")\n","\n","  print(f\"{shell_class}: AP={100 * ap:.2f}%, F1={100 * peakf1.f1:.2f}%\")\n","\n","plt.title(\"Precision-Recall curve for each class at optimal nms threshold\")\n","ax.set_xlabel(\"Recall\")\n","ax.set_ylabel(\"Precision\")\n","ax.legend()\n","\n","fig.savefig(f\"{output_path}classesPR.png\")\n","plt.close()"]},{"cell_type":"markdown","source":["# Merged for continuous running"],"metadata":{"id":"n5yBJXDfTAB8"}},{"cell_type":"code","source":["classless_options = [True, False]\n","N_options = [1, 5, 10, 20, 50]\n","use_all_generated_image_embeddings_options = [True, False]\n","\n","options_matrix = list(itertools.product(classless_options, N_options, use_all_generated_image_embeddings_options))\n","\n","use_text_queries = False\n","use_negative_queries = False #text only\n","for classless, N, use_all_generated_image_embeddings in options_matrix:\n","  print(f\"Running classless: {classless}, N:{N}, use_all_generated_image_embeddings:{use_all_generated_image_embeddings}\")\n","  class_string = \"classless\" if classless else \"classed\"\n","  neg_string = \"use_negative_queries\" if use_negative_queries else \"only_positive_queries\"\n","  text_string = \"text\" if use_text_queries else \"img\"\n","  extra_img_string = f\"full_{N}_\" if (not use_text_queries and use_all_generated_image_embeddings) else f\"{N}_\"\n","\n","  output_path = f\"{output_path_base}/owlvit/{text_string}/{class_string}/{neg_string}/{extra_img_string}\"\n","\n","  saves_path = f\"{saves_path_base}/owlvit/{text_string}/{class_string}/{neg_string}/{extra_img_string}\"\n","\n","  if use_text_queries:\n","    annotations:pd.DataFrame = pd.concat([image.annotations for image in dataset])\n","\n","    if classless:\n","      positive_text_queries = (\"shell\", \"seashell\") + tuple(annotations.class_label.unique())\n","    else:\n","      positive_text_queries = tuple(annotations.class_label.unique())\n","    if use_negative_queries:\n","      negative_text_queries = (\"rock\", \"sand\")\n","    else:\n","      negative_text_queries = ()\n","    queries = negative_text_queries + positive_text_queries\n","    query_embeddings = model.embed_text_queries(queries)\n","\n","    test_dataset = dataset\n","\n","  else:\n","    annotations:pd.DataFrame = pd.concat([image.annotations for image in dataset])\n","\n","    test_dataset = dataset.copy()\n","    query_dataset = set()\n","\n","    # we take out the query images from which we make the query embeddings\n","    # if a class has less than N annotations we dont use it\n","    annotations_per_class = annotations['class_label'].value_counts()\n","    removed_classes = []\n","    queries_dict = {}\n","    for annotation_class, _ in annotations_per_class.items():\n","      queries_dict[annotation_class] = []\n","    for annotation_class, amount in reversed(list(annotations_per_class.items())): #loop over counts in reverse\n","      if amount < N:\n","        removed_classes.append(annotation_class)\n","      else:\n","        if len(queries_dict[annotation_class]) > N:\n","          continue #we dont need to generate any more queries for classes that are already sufficiently populated\n","\n","        for image in test_dataset:\n","          if annotation_class in list(image.annotations.class_label.values):\n","            query_dataset.add(image)\n","            img_query_embeddings = get_all_query_embeddings(image)\n","            for query_class, embedding in img_query_embeddings:\n","              queries_dict[query_class].append(embedding)\n","          if len(queries_dict[annotation_class]) > N:\n","            break\n","\n","    for image in query_dataset:\n","      test_dataset.remove(image)\n","\n","    annotations = pd.concat([image.annotations for image in test_dataset])\n","\n","    for c in removed_classes:\n","      annotations = annotations[annotations['class_label'] != c]\n","      queries_dict.pop(c)\n","\n","\n","    query_embeddings_list = []\n","    queries = []\n","\n","    for key, value in queries_dict.items():\n","      queries.append(key)\n","      if use_all_generated_image_embeddings:\n","        query_embeddings_list.append(np.mean(value, axis=0))\n","      else:\n","        query_embeddings_list.append(np.mean(value[:N], axis=0))\n","\n","    query_embeddings = np.array(query_embeddings_list)\n","\n","\n","\n","  if classless:\n","      annotations['class_label'] = 'object'\n","\n","\n","  if use_text_queries:\n","    # This isnt all that useful, but it is used to count the amount of boxes that the model gives.\n","    # Could this be done with our own images or even a null-image?\n","    # Sure, but why bother if it already works (doesnt take that long either)\n","\n","    IMAGE_DIR = 'gs://scenic-bucket/owl_vit/example_images'\n","    %matplotlib inline\n","\n","    images = {}\n","\n","    for i, filename in enumerate(tf.io.gfile.listdir(IMAGE_DIR)):\n","      with tf.io.gfile.GFile(os.path.join(IMAGE_DIR, filename), 'rb') as f:\n","        image = mpl.image.imread(\n","            f, format=os.path.splitext(filename)[-1])[..., :3]\n","      if np.max(image) <= 1.:\n","        image *= 255\n","      images[i] = image\n","    IMAGE_ID =   2\n","    image = images[IMAGE_ID]\n","\n","    _, _, boxes = model.embed_image(image)\n","\n","  else:\n","    test_image_annotated = dataset[1]\n","    _, _, boxes = model.embed_image(test_image_annotated.npimage)\n","\n","  detections:pd.DataFrame = pd.DataFrame(index=range(len(test_dataset) * len(boxes)), columns=['image', 'class_label', 'id', 'x_top_left', 'y_top_left', 'width', 'height', 'confidence', 'area'])\n","\n","  index = 0\n","\n","  for q in range(len(test_dataset)):\n","      test_image_annotated = test_dataset[q]\n","      test_image_name = test_image_annotated.filename.split(\".\")[0]\n","      test_image = test_image_annotated.npimage\n","\n","      _, _, boxes = model.embed_image(test_image)\n","      query_index, scores = model.get_scores(test_image, query_embeddings, len(queries))\n","\n","      zipped_scores = zip(query_index, scores, boxes) # tuples: (label_id, score, [x, y, w, h])\n","      for i in range(len(boxes)):\n","          box = boxes[i]\n","          score = scores[i]\n","          class_label = queries[query_index[i]]\n","          x_top_left, y_top_left, width, height = box\n","          detections.loc[index] = [test_image_name, class_label, i, ((x_top_left - width/2) * test_image_annotated.width), ((y_top_left - height/2) * test_image_annotated.width) , width * test_image_annotated.width, height * test_image_annotated.width, score, width * height * test_image_annotated.height * test_image_annotated.width]\n","          index += 1\n","\n","          #new_row:pd.DataFrame = pd.DataFrame(columns=detections.columns, data=[[test_image_name, class_label, i, x_top_left, y_top_left, width, height, score]])\n","\n","          #detections.iloc[q*len(queries)+i] = new_row\n","\n","  # add area column\n","  detections['image'] = detections['image'].astype('category')\n","  detections['area'] = detections['width'] * detections['height']\n","\n","  if use_negative_queries and use_text_queries:\n","    for q in negative_text_queries:\n","        detections = detections[detections['class_label'] != q]\n","  if classless:\n","    for q in queries:\n","      detections.loc[detections['class_label'] == q, 'class_label'] = 'object'\n","\n","  bb.io.save(detections, \"pandas\", saves_path + \"unprocessed_detections.pkl\")\n","\n","  detections = bb.io.load(\"pandas\", saves_path + \"unprocessed_detections.pkl\")\n","\n","  detections['iou'] = 0.0\n","  lowest_confidence = 0.01 if classless else 0.10\n","  lowest_confidence = lowest_confidence if use_text_queries else 0.6\n","\n","  detections = filter_by_confidence(detections, lowest_confidence)\n","\n","  index = 1\n","  total_len = len(detections.image.unique())\n","\n","  for image in detections.image.unique():\n","    img_detections = get_detections_for_image(detections, image)\n","    index += 1\n","    calc_iou(detections, img_detections)\n","\n","  bb.io.save(detections, \"pandas\", saves_path + \"processed_detections.pkl\")\n","\n","  detections = bb.io.load(\"pandas\", saves_path + \"processed_detections.pkl\")\n","\n","  detections['height'] = detections['height'].astype(float)\n","  detections['confidence'] = detections['confidence'].astype(float)\n","  detections['area'] = detections['area'].astype(float)\n","  detections['iou'] = detections['iou'].astype(float)\n","  detections['x_top_left'] = detections['x_top_left'].astype(float)\n","  detections['y_top_left'] = detections['y_top_left'].astype(float)\n","  detections['width'] = detections['width'].astype(float)\n","\n","\n","  annotations['x_top_left'] = annotations['x_top_left'].astype(float)\n","  annotations['y_top_left'] = annotations['y_top_left'].astype(float)\n","  annotations['width'] = annotations['width'].astype(float)\n","  annotations['height'] = annotations['height'].astype(float)\n","  annotations['occluded'] = annotations['occluded'].astype(float)\n","  annotations['truncated'] = annotations['truncated'].astype(float)\n","  annotations['lost'] = annotations['lost'].astype(bool)\n","  annotations['difficult'] = annotations['difficult'].astype(bool)\n","  annotations['ignore'] = annotations['ignore'].astype(bool)\n","\n","  iou_thresholds = [round(0.1 * x, 2) for x in range(6,11)]\n","  score_thresholds = [round(x/10, 6) for x in range(round(lowest_confidence * 10), 10)] #lower than the lowest score is meaninless ofc\n","\n","  options_matrix = list(itertools.product(iou_thresholds, score_thresholds))\n","\n","  save_images = True\n","  image_amount = 15\n","  if image_amount > len(test_dataset):\n","    image_amount = len(test_dataset)\n","\n","\n","\n","  if save_images:\n","    for iou, score in options_matrix:\n","      det_iou = filter_by_iou(detections, iou)\n","      det = filter_by_confidence(det_iou, score)\n","\n","      img_path = f\"{output_path}iou{iou}/score{score}/\"\n","      create_folder_if_not_exists(img_path)\n","      for image in test_dataset[0:image_amount]:\n","        boxed:Image = draw_boxes(image, annotations, line=False)\n","        boxed = draw_boxes(image, det, draw_legend=True, manual_image=boxed)\n","        with open(img_path + image.filename, \"w\") as imfp:\n","          boxed.save(imfp)\n","\n","  max_ap = 0\n","  max_ap_nms = 0\n","  max_ap_pr = None\n","  max_ap_fscore = 0\n","\n","  max_fscore = None\n","  max_fscore_f1 = 0\n","  max_fscore_nms = 0\n","  max_fscore_pr = None\n","  max_fscore_ap = 0\n","\n","  fig, ax = plt.subplots(figsize=(10,10))\n","\n","  for iou_threshold in iou_thresholds:\n","\n","    det = filter_by_iou(detections, iou_threshold)\n","    pr = bb.stat.pr(det, annotations, threshold=0.5, smooth=True)\n","    ap = bb.stat.ap(pr)\n","    fscore = bb.stat.fscore(pr)\n","    peakf1 = bb.stat.peak(fscore)\n","    ax.plot(pr['recall'], pr['precision'], label=f\"nms={iou_threshold}, AP={100 * ap:.2f}%, F1={100 * peakf1.f1:.2f}%\")\n","\n","    print(f\"nms={iou_threshold}, AP={100 * ap:.2f}%, F1={100 * peakf1.f1:.2f}%\")\n","\n","    if ap > max_ap:\n","        max_ap = ap\n","        max_ap_nms = iou_threshold\n","        max_ap_pr = pr\n","        max_ap_fscore = peakf1.f1\n","    if peakf1.f1 > max_fscore_f1:\n","        max_fscore_f1 = peakf1.f1\n","        max_fscore = peakf1\n","        max_fscore_nms = iou_threshold\n","        max_fscore_pr = pr\n","        max_fscore_ap = ap\n","        topf1 = bb.stat.point(pr, peakf1.f1)\n","\n","  plt.title(\"Precision-Recall curve for different nms values\")\n","  ax.set_xlabel(\"Recall\")\n","  ax.set_ylabel(\"Precision\")\n","  ax.legend()\n","\n","  fig.savefig(f\"{output_path}iouPR.png\")\n","  plt.close()\n","\n","  print(f\"best ap: {max_ap * 100}%, best fscore: {max_fscore_f1 * 100}%\")\n","  print(f\"maximize ap: nms={max_ap_nms}\")\n","  print(f\"maximize fscore: nms={max_fscore_nms}\")\n"],"metadata":{"id":"C3TsheN3S_Je"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"last_runtime":{"build_target":"//learning/deepmind/public/tools/ml_python:ml_notebook","kind":"private"},"machine_shape":"hm","private_outputs":true,"provenance":[{"file_id":"https://github.com/google-research/scenic/blob/main/scenic/projects/owl_vit/notebooks/OWL_ViT_inference_playground.ipynb","timestamp":1691078650557},{"file_id":"1kBebGRuMcABXiprw6IEAxpbKAXNO4EOQ","timestamp":1651575080312},{"file_id":"https://github.com/google-research/scenic/blob/main/scenic/common_lib/colabs/scenic_playground.ipynb","timestamp":1650960476931}],"history_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}