%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%                                                                 %
%                            CHAPTER                              %
%                                                                 %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

\chapter{Literature Review}
In this chapter, we will review the state of the art in the field of object counting. We will study the techniques commonly used for counting and go in more depth about the topic of few-shot object detection and why it should be applied to our problem. Finally, we will discuss the metrics used to evaluate the performance of the models.

\section{(Crowd) Counting}
Counting networks are an established concept in machine learning as numerous papers tackle the issue of counting humans, cars, animals or cells. What those have in common is that they only encompass a small set of possible categories to count and that, as they have a large real-life use, large annotated datasets exist like ShanghaiTech\cite{Shanghaitech} and COWC\cite{COWC}. The problem we are trying to solve is a bit different as we want to count a large set of objects and yet we don't have a large dataset to train on.

The methodology behind heuristic counting networks has three big streams\cite{s22145286}. The first applies a detection method to the image and then counts the number of detected objects. Many different detection methods can be used, from looking for characteristic features to matching the shape of the objects. The second takes a more global approach by first extracting features, textures, gradients and other information from the image as a whole and then using those to count the objects. The third method is not used on static images, but on video. It assumes that the objects are moving in clusters and uses that to predict the movement of the objects and improve detection.

Out of those three methods, the third one is not applicable to our problem as we are trying to detect unmoving objects in a still image.
Both the first and second methods are applicable to our problem, however, both have the problem of requiring a large dataset to train on. We will have to use a method that doesn't require a large dataset, which is where few-shot learning comes in. In the domain of few-shot learning the first method, object detection, is the most common. In the next section, we will go more in-depth about few-shot object detection. %FUTURE WORK: try the second method.

%https://www.mdpi.com/1424-8220/22/14/5286

%https://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/papers/Ranjan_Vicinal_Counting_Networks_CVPRW_2022_paper.pdf

\section{Few-shot object detection}
%https://arxiv.org/abs/2112.11699
%https://ieeexplore-ieee-org.kuleuven.e-bronnen.be/stamp/stamp.jsp?tp=&arnumber=1597116
%add a reference to the first paper that did it
Few-shot object detection is a technique that has been gaining popularity in the last few years, but interest in making a model to classify without a big annotated dataset appeared as early as 2008 with zero-shot learning in \citet{aaai08-132}. It allows us to train a model with few annotated images, which is useful in scenarios where it isn't possible to get a large annotated dataset. Few-shot attempts to mirror the way humans learn, during our life we come across many new objects and we are able to recognize them even though we only saw them a few times. We do this by drawing on our knowledge of other objects and using that to recognize the new object\cite{biederman1987recognition}. 

Different approaches to few-shot learning vary on a few characteristics
\begin{itemize}
	\item The type of architecture used
	\item The amount and type of data used
\end{itemize}

In this section, we will go over the different options for each of these characteristics.

\subsection{Method}

For the method there are two options, transfer learning and meta-learning. Each of these has its own advantages and disadvantages. %add metric learning and data augmentation

\subsubsection*{Transfer learning}

Transfer learning is a technique that has been used for a long time in machine learning. It allows us to use a model that has been trained on a large dataset as a base and, with a few changes to mitigate the small size of the novel dataset in few-shot learning, finetune (the last layers) on a novel dataset. The advantages of this method are that it is relatively easy to implement and it is fast. One of the problems with this method is that, because of the small size of the novel dataset, the Region proposal network (RPN) can not be properly trained and can sometimes completely miss the novel object classes. Mitigations for this problem do, however, exist. \cite{DBLP:journals/corr/abs-2011-10142, VU2022104398, DBLP:journals/corr/abs-2105-09491, DBLP:journals/corr/abs-2103-05950,rs14143255}.

\subsubsection*{Meta-learning}

Meta-learning learns on a higher order of abstraction. Instead of learning how to detect objects it learns how to learn to detect objects. It does this with the help of a large dataset, by learning how to best extract and differentiate the features of its classes. Due to the dataset being large, this can then be applied to a novel dataset. It is best if the novel dataset is similar to the large dataset, as it will be able to generalize better. Practically meta-learning is most commonly done by introducing a support branch\cite{few-shot-comprehensive-survey}, displayed in figure \ref{fig:2_dualbranchmeta}. An advantage of this method is that it is better at the detection of alike novel classes due to the meta loss, a loss function on the support branch. %needs citation
The disadvantages are its complexity and that it is slower to train than transfer learning due to the aforementioned support branch. As the support branch is computed once after training, it is not significantly slower during inference.
%needs sauces

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{2_dualbranchmeta}
	\caption{\label{fig:2_dualbranchmeta} Dual branch meta learning. Image from \citet{few-shot-comprehensive-survey}.}
\end{figure}

\subsection{Data}

The amount and type of data used in few-shot learning is also an important factor. A common way to describe a few shot task is "N-Way, K-Shot". Where N is the number of classes and K is the number of examples per class. The more examples we have per class, the easier it is to learn. The larger the number of classes the harder. Models are often benchmarked with increasing K to see how they perform with values of K often set at (2,) 5, 10 and 30. When the amount of images decreases even further we enter a whole new category of few-shot learning, one-shot learning and zero-shot learning(which is out of scope for this paper). Also important is the type of data used. The three ways a model can learn are supervised, semi-supervised and unsupervised. Respectively working with a fully annotated dataset, a partially annotated dataset and an unannotated dataset.

\subsubsection*{One-shot learning}

While one-shot learning is not a new concept, applying it to object detection is hard. Early applications used a siamese backbone, as is often seen in other one-shot applications, but this was not very successful \citep{One-shot-siamese}. Recently however OWL-ViT\citep{owlvit} improved one-shot object detection by a large margin. It first uses a large dataset of web-derived image-text pairings to pre-train a vision and a text encoder. The model is then modified to enable detection and Open-vocabulary classification. Lastly, it is finetuned for detection. The model can take a text or image-derived embedding and find matching objects in the query image. As the model can take image input it can be used for one-shot object detection. It reaches 41.8 mAP for one-shot object detection on the COCO dataset. %to sota?


\section{Metrics}
In machine learning, it is important to test the model after training, to evaluate its performance. To test the model's accuracy a part of the initial dataset is split off into a test set and never used when training. As we have the ground truth for the test set we can compare it with the network output to find if the detections are correct. 

To find if the model output matches the expected output we use the Intersection over Union (IoU) metric. This compares the area of the input and output bounding boxes for each detection by dividing the area of intersection by the area of union. If the IoU, calculated as shown in \ref{eq:IoU}, is above a threshold (th) it is considered a detection.

\begin{figure}[h]
	\centering
	\input{fig/2_IoU_tikz.tex}
	\caption{\label{fig:2_IoU_det} IoU and class match to find the type of detection.}
\end{figure}

\begin{equation}
	\text{IoU} = \frac{\text{area of intersection}}{\text{area of union}}
	\label{eq:IoU}
\end{equation}

Each detection can be put into one of four categories, based on if and how well it matches the ground truth, listed below.

\begin{itemize}
	\item True positive: The model correctly detects an object and the IoU is above the threshold.
	\item False positive: The model detects an object but the IoU is below the threshold or the model mislabels the object.
	\item False negative: The model does not detect an object but it should have.
	\item True negative: The model does not detect an object and it should not have, this is not used in the metrics as it is not very useful.
\end{itemize}

Using this a few key metrics can be calculated. The main metrics we will use are precision, recall and their derivatives. Precision (\ref{eq:precision}) is the ratio of true positives to the total number of positives. Recall (\ref{eq:recall}) is the ratio of true positives to the total number of detectable positives.



\begin{equation}
	Precision = \frac{TruePositives}{True Positives + False Positives}
	\label{eq:precision}
\end{equation}

\begin{equation}
	Recall = \frac{True Positives}{True Positives + False Negatives}
	\label{eq:recall}
\end{equation}

The threshold between high and low confidence can be chosen, a high threshold will result in a low recall but high precision. A low threshold will result in a high recall but low precision. Plotting the precision and recall against the threshold results in a precision-recall curve.

Averaging the precision across all recall levels results in the average precision (AP) metric. Averaging the AP over all classes results in the mean average precision (mAP) metric. The mAP is the most common metric used to evaluate object detection models.

\section{State of the art}
In this section, we will go over the state of the art in few-shot object detection with both transfer learning and meta-learning.
\subsection{Transfer learning}
Transfer learning, as previously discussed, takes a model trained on a large dataset and finetunes it on a novel dataset. In this section, we will go over the state of the art in few-shot object detection with transfer learning.

\subsubsection{A Low-Shot Transfer Detector for Object Detection(\citet{LSTD})}
\citet{LSTD} proposes a new architecture, designed to mitigate transfer learning difficulties in few-shot detection by combining Single Shot Detector (SSD) and Faster R-CNN into Low-Shot Transfer Detector (LSTD). The LSTD architecture is shown in figure \ref{fig:2_LSTD}. In LSTD bounding box regression is first performed according to SSD. The advantage of using SSD is that all classes share a single bounding box regressor, making it suitable for pretraining on a large dataset. The object detection is performed according to a modified Faster R-CNN. Faster R-CNN is modified, replacing the fully connected layers with two convolutional layers, to reduce overfitting.

On the ImageNet2015 dataset, it outperforms Faster R-CNN and SSD. On the VOC2007 and the VOC2010 dataset, it outperforms weakly and semi-supervised object detection methods when the number of training images is above 1. Note that both the weakly and the semi-supervised approaches require the full training set, so LSTD outperforms them with merely 0.4\% training data.

\begin{figure}[h]
	\centering
	\includegraphics[width=1\textwidth]{2_LSTD_simple.jpg}
	\caption{\label{fig:2_LSTD} LSTD architecture. Image from \citet{LSTD}.}
\end{figure}

\subsubsection{DeFRCN: Decoupled Faster R-CNN for Few-Shot Object Detection (\citet{DeFRCN})}
\citet{DeFRCN} extends Faster R-CNN\cite{fasterrcnn} by introducing Gradient Decoupled Layers (GDL) and Prototypical Calibration Block to create Decoupled Faster R-CNN (DeFRCN). The DeFRCN architecture is shown in figure \ref{fig:2_DeFRCN}.

During forward propagation, the GDLs apply an affine transformation layer to enhance feature representation and perform forward decoupling. During backward propagation, the GDL takes the gradient from the subsequent layer, multiplies it with a constant and passes it to the preceding layer. This reduction ensures that the update speed of the backbone is slower than the update speed of the connected network, this prevents overfitting due to the small dataset. In practice, the gradient between the backbone and the RPN is set to 0 as the RPN is largely class-agnostic, and the gradient between the RPN and the RCNN Head is set to 0.75 during base training and 0.01 during fine-tuning.

The Prototypical Calibration Block (PCB) is a metric-based score refinement module. It aims to eliminate high-scored false positives and remedy low-scored massing samples. It consists of a strong classifier from an ImageNet pre-trained model, an RoI-align layer and a prototype bank. In practice it first extracts the original image feature maps, it then employs RoI-align with ground-truth boxes to produce instance representations. Based on these features the support set is shrunk to a prototype bank. When given an object proposal from the fine-tuned few-shot detector, it first performs RoI-align on the predicted box to generate the object features. It then calculates the cosine similarity between the object features and the prototype bank. Finally, a weighted aggregation between the original score and the cosine similarity is performed. The weight is fixed to 0.5 in practice, resulting in equal weight between the original score and the cosine similarity. As the PCB module is offline and does not require any additional training, it can be easily integrated into any existing object detection framework.

On both COCO and VOC datasets, DeFRCN outperforms the state of the art in all but one combination of novel classes and training images.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.5\textwidth]{2_DeFRCN.jpg}
	\caption{\label{fig:2_DeFRCN} DeFRCN architecture. Image from \citet{DeFRCN}.}
\end{figure}

\subsection{Meta-learning}
Meta-learning is a learning paradigm that aims to learn how to learn. It is a form of transfer learning that learns to transfer knowledge from one task to another. In this section, we will go over the state of the art in few-shot object detection with meta-learning.

\subsubsection{Meta R-CNN: Towards General Solver for Instance-level Low-shot Learning (\citet{meta-rcnn})}
\citet{meta-rcnn} proposes a meta-learning approach to few-shot object detection. It extends
Faster /Mask R-CNN\cite{maskrcnn} by introducing a Predictor-head Remodeling Network (PRN). PRN receives few-shot objects drawn from base and novel classes with their bounding boxes or masks, inferring class-attentive vectors corresponding to the classes that few-shot input objects belong to. These class-attentive vectors are then applied to take channel-wise attention on each RoI feature map, after which a binary detection outcome is predicted. The PRN architecture is shown in figure \ref{fig:2_metarcnn}.

\begin{figure}[h]
	\centering
	\includegraphics[width=1\textwidth]{2_metarcnn.jpg}
	\caption{\label{fig:2_metarcnn} Meta R-CNN architecture with PRN. Image from \citet{meta-rcnn}.}
\end{figure}

\subsubsection{Meta-DETR: Image-Level Few-Shot Detection with Inter-Class Correlation Exploitation (\citet{MetaDETR})}
\citet{MetaDETR} proposes the first few shot object detector to work on the image level, shown in \ref{fig:2_imagelevelprediction}. It extends Deformable DETR with a Correlational Aggregation Network (CAN) to create Meta-DETR, shown in figure \ref{fig:2_metadetr}. The CAN architecture is shown in figure \ref{fig:2_CAN}. 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.4\textwidth]{2_imagelevelprediction.jpg}
    \caption{\label{fig:2_imagelevelprediction} Image level detection. Image from \citet{MetaDETR}.}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{2_metadetr.jpg}
    \caption{\label{fig:2_metadetr} Meta-DETR architecture. Image from \citet{MetaDETR}.}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.4\textwidth]{2_can.jpg}
    \caption{\label{fig:2_CAN} CAN architecture. Image from \citet{MetaDETR}.}
\end{figure}

As shown in fig \ref{fig:2_CAN} the query and support features, extracted by ResNet-101, are first processed by a weight-shared multi-head attention module, encoding them into the same embedding space. Then the prototype for each support class is obtained by applying RoIAlign, followed by average pooling on the support features to get the support prototypes. Feature and encoding matching is then performed on the support prototypes and query features. The sum of the matching scores is then fed to a feed-forward network to obtain the final output.

\section{Conclusion}
In this chapter we have studied object counting, narrowing it down to few-shot object detection to then count the detections. We went into more detail regarding the different ways to implement few-shot learning to detect objects. The sub-field of meta-learning shows the most promise for our problem as it is better at learning the difference between two alike novel classes compared to transfer learning.